<h3 data-label="990860" class="ltx_title_subsubsection">Cropping and Resampling</h3><div>For steps one and two, each of the 121 raw raster images (39 snapshots of snow depth at 3m, 39 snapshots of snow depth at 50m, 39 snapshots of SWE at 50m, and 4 snowoff rasters) was given assigned to a batch job in which it was first cropped and corrected and then resampled to 3 lower resolutions. 3m snowon data (snow depth and SWE) were resampled to 10, 30, and 50m. 50m snowon data was resampled to 100, 300, and 500m. 50m data was thus obtained both via raw rasters and via resampled 3m rasters to facilitate validation of our resampling techniques. In the case of snowoff rasters, since these were only provided at 3m, they were resampled to 10, 30, 50, 100, 300, and 500m. Node memory constraints in the Yellowstone cluster, where the majority of our core-hour allocation was located, prevented full CPU utilization for cropping and resampling of the 3m images. Thus, a parallelization strategy was devised such that one job was submitted for cropping and resampling the 3m rasters, using 8 CPUs per node and a separate was submitted for cropping and resampling the 50m rasters, in which all 16 CPUs per node were allocated a raster to process. The parallel processing was invoked using NCAR's "command file" framework, which allows for flexible Multiple Program Multiple Data (MPMD) parallelism as long as each "command" is independent. To use this framework, we created a file with one command per line and then passed this script as an argument to the batch submission script. The script then called a custom <i>mpirun.lsf</i> executable and passed this "command file" as an argument.&nbsp;We were using a SPMD approach, so did not need the extra MPMD flexibility; nevertheless, this framework provided the simplest way to call a given script independently on multiple datasets within the batch environment.</div><h3 data-label="148321" class="ltx_title_subsubsection">DEM Transformations</h3><div>With the exception of the irradiance and MUHA outputs, each of the DEM transformations was essentially a neighborhood analysis performed at each pixel and, as such, could reasonably be parallelized to independent processes via a domain decomposition with no communication necessary. The two exceptions, however, involved horizon angle calculations using a greater extent of the underlying DEM. These two transformations were multiple orders of magnitude more expensive and thus the highest resolution (3m) versions of these transformations were the rate limiting step within our data preprocessing pipeline. We were not able to perform these steps in serial on a single processor due to 24-hour wall-clock time constraints, but a rough estimate of 72 hours for the irradiance calculation seems reasonable. Our tool for these transformations was GRASS GIS, a software platform with optimized algorithms for these terrain analyses yet built primarily for desktop/SMP systems. There is demonstrated interest in adapting GRASS to HPC environments <cite class="ltx_cite raw v1">\cite{akhter_grass_2010}</cite>, but of yet, this is manifested largely as a series of disjointed academic manuscripts and Wiki-based instruction articles, rather than standardized implementations. Thus, we had two options available for parallelization of these tasks on our HPC system.</div><div>First, we could have written our own parallelized algorithm, either adapting GRASS itself or starting from scratch in a language of our choice, borrowing the algorithm currently implemented in GRASS. While this would be a valuable exercise, it was likely the scope of an entire project in itself and, in the case of adapting GRASS itself, one that many others have been struggling with. Second, we could parallelize the generation of intermediate files necessary for these algorithms using the batch system and job dependencies to facilitate this process. As there was precedent for this form of parallelization of GRASS algorithms on HPC systems, we opted for this option. For the irradiance cal</div><h2 data-label="948109" class="ltx_title_subsection">Regression</h2><div>We attempted three regression techniques to model the watershed.</div>